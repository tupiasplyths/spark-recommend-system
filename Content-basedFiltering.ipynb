{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Start Spark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project\") \\\n",
    "    .master(\"spark://10.10.28.60:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "    # .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    # .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    \n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml.feature import IDF\n",
    "from operator import add\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data Preparation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "movies = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/movies.parquet')\n",
    "ratings = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/ratings.parquet')\n",
    "tags = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/tags.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Create a temporary view for to access data using SQL-like queries\n",
    "df_movies = movies\n",
    "df_ratings = ratings\n",
    "df_tags = tags\n",
    "df_movies.createOrReplaceTempView(\"movies\")\n",
    "df_ratings.createOrReplaceTempView(\"ratings\")\n",
    "df_tags.createOrReplaceTempView(\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Load the tags data from the database and preprocessing tags data by removing space and concatenating them\n",
    "tags_text = spark.sql(\"SELECT movieId, CONCAT(REPLACE(TRIM(tag), ' ', ''),' ') as tag FROM tags\")\n",
    "\n",
    "#Group by tag by movieId and sort\n",
    "tags_text_rdd = tags_text.rdd\n",
    "tags_by_movie_rdd = tags_text_rdd.map(tuple).reduceByKey(add)\n",
    "tags_by_movie_df = spark.createDataFrame(tags_by_movie_rdd).orderBy(\"_1\", ascending = True)\n",
    "\n",
    "tags_by_movie_df = tags_by_movie_df  \\\n",
    "                            .withColumnRenamed('_1', 'movie_id') \\\n",
    "                            .withColumnRenamed('_2', 'tag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_path = 'hdfs://master5:9000/user/dis/output-3'\n",
    "output_path = 'hdfs://master5:9000/user/dis/output-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Create a pipeline for calculating the vector word\n",
    "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'tag', outputCol = 'token')\n",
    "stopWordsRemover = StopWordsRemover(inputCol = 'token', outputCol = 'nostopwrd')\n",
    "countVectorizer = CountVectorizer(inputCol=\"nostopwrd\", outputCol=\"rawFeature\")\n",
    "iDF = IDF(inputCol=\"rawFeature\", outputCol=\"idf_vec\")\n",
    "word2Vec = Word2Vec(vectorSize = 50, minCount = 5, inputCol = 'nostopwrd', outputCol = 'word_vec', seed=123)\n",
    "vectorAssembler = VectorAssembler(inputCols=['idf_vec', 'word_vec'], outputCol='comb_vec')\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, countVectorizer, iDF, word2Vec, vectorAssembler])\n",
    "\n",
    "#Fit the pipeline to the tags data\n",
    "pipeline_mdl = pipeline.fit(tags_by_movie_df)\n",
    "\n",
    "#save the pipeline model\n",
    "pipeline_mdl.write().overwrite().save(model_path + 'pipe_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Load the previous saved pipeline model\n",
    "pipeline_mdl = PipelineModel.load(model_path + 'pipe_txt')\n",
    "\n",
    "#Transform the tags data using the pre-trained pipeline\n",
    "tags_by_movie_trf_df = pipeline_mdl.transform(tags_by_movie_df)\n",
    "\n",
    "#Save the dataframe to parquet for loading without text transformation\n",
    "movieId_vecs= tags_by_movie_trf_df.select('movie_id', 'word_vec')\n",
    "movieId_vecs.write.mode('overwrite').parquet(output_path + 'movieId_vecs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CosineSim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "    vec1 (numpy.ndarray): First vector.\n",
    "    vec2 (numpy.ndarray): Second vector.\n",
    "    \n",
    "    Returns:\n",
    "    float: Cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Read data from a Parquet file\n",
    "movieId_vecs = spark.read.parquet(output_path + 'movieId_vecs.parquet')\n",
    "\n",
    "all_movieId_vecs = movieId_vecs.select('movie_id', 'word_vec').rdd.map(lambda x: (x[0], x[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def getSimilarMovies(m_id, sim_mos_limit=5):\n",
    "    \"\"\"\n",
    "    This function finds the most similar movies for a given set of input movie IDs.\n",
    "    \n",
    "    Args:\n",
    "        m_ids (list): A list of movie IDs for which to find similar movies.\n",
    "        sim_mos_limit (int, optional): The maximum number of similar movies to return for each input movie. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A DataFrame containing the similar movies, their similarity scores, and the input movie ID.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "                            StructField(\"movie_id\", IntegerType(), True)\n",
    "                            ,StructField(\"score\", IntegerType(), True)\n",
    "                            ,StructField(\"input_movie_id\", StringType(), True)\n",
    "                        ])\n",
    "\n",
    "    similar_movies_df = spark.createDataFrame([], schema)\n",
    "    print(m_id)\n",
    "    m_id = int(m_id)\n",
    "    input_vec = movieId_vecs.select('word_vec')\\\n",
    "                .filter(movieId_vecs['movie_id'] == m_id)\\\n",
    "                .collect()[0][0]\n",
    "    start_time = time.time()\n",
    "    similar_movie_rdd = sc.parallelize((i[0], float(CosineSim(input_vec, i[1]))) for i in all_movieId_vecs)\n",
    "    print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    similar_movie_df = spark.createDataFrame(similar_movie_rdd) \\\n",
    "            .withColumnRenamed('_1', 'movie_id') \\\n",
    "            .withColumnRenamed('_2', 'score') \\\n",
    "            .orderBy(\"score\", ascending = False)\n",
    "\n",
    "    similar_movie_df = similar_movie_df.filter(col(\"movie_id\") != m_id).limit(sim_mos_limit)\n",
    "    similar_movie_df = similar_movie_df.withColumn('input_movie_id', lit(m_id))\n",
    "\n",
    "    similar_movies_df = similar_movies_df \\\n",
    "                                .union(similar_movie_df)\n",
    "\n",
    "    return similar_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def getMovieDetails(in_mos):\n",
    "    \"\"\"\n",
    "    This function retrieves additional details for a set of input movies.\n",
    "    \n",
    "    Args:\n",
    "        in_mos (pyspark.sql.DataFrame): A DataFrame containing movie IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A DataFrame containing the input movie IDs, movie titles, and movie genres.\n",
    "    \"\"\"\n",
    "    a = in_mos.alias(\"a\")\n",
    "    b = df_movies.alias(\"b\")\n",
    "\n",
    "    #Join the movie DataFrame to get titles and genres to the correspond movieId\n",
    "    return a.join(b, col(\"a.movie_id\") == col(\"b.movieId\"), 'inner') \\\n",
    "             .select([col('a.'+xx) for xx in a.columns] + [col('b.title'),col('b.genres')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Get n movies in movieId_vecs\n",
    "number_movies = movieId_vecs.select(\"movie_id\").limit(5).collect()\n",
    "for row in number_movies:\n",
    "    movie_id = row.movie_id\n",
    "    print(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mids = []\n",
    "for row in number_movies:\n",
    "    mids.append(str(row.movie_id))\n",
    "print(mids)\n",
    "#Get 5 movies recommendation for n movies\n",
    "for i in range(0,len(mids)):\n",
    "    print('\\ninput movies details:')\n",
    "    df_movies.select('movieId', 'title', 'genres') \\\n",
    "        .filter(df_movies.movieId == mids[i]).show(truncate=False)\n",
    "    try:\n",
    "        sims = getMovieDetails(getSimilarMovies(mids[i]))\n",
    "        print(f'Top 5 similar movies for {mids[i]} each input movies are:\"')\n",
    "        display(sims.select('input_movie_id', 'movie_id', 'title', 'score').toPandas())\n",
    "    except IndexError as e:\n",
    "        print(f\"Error processing movie ID {mids[i]}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContentRecoms(u_id, sim_mos_limit=5):\n",
    "    \"\"\"\n",
    "    Generates content-based movie recommendations for a given user.\n",
    "\n",
    "    Args:\n",
    "        u_id (str): The ID of the user to generate recommendations for.\n",
    "        sim_mos_limit (int): The maximum number of similar movies to retrieve for each of the user's reviewed movies.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A DataFrame containing the movie IDs, titles, and genres of the recommended movies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # select movies having rating >= 3\n",
    "    query = \"\"\"\n",
    "    SELECT distinct movieId as movie_id FROM ratings\n",
    "    where rating >= 3.0\n",
    "    and userId = \"{}\"\n",
    "    \"\"\".format(u_id)\n",
    "\n",
    "    usr_rev_mos = sqlContext.sql(query)\n",
    "\n",
    "    # from these get sample of 5 movies\n",
    "    usr_rev_mos = usr_rev_mos.sample(False, 0.5).limit(5)\n",
    "\n",
    "    usr_rev_mos_det = getMovieDetails(usr_rev_mos)\n",
    "\n",
    "    # show the sample details\n",
    "    print('\\nMovies previously reviewed by user {}'.format(u_id))\n",
    "    usr_rev_mos_det.select(['movie_id', 'title', 'genres']).show(truncate = False)\n",
    "\n",
    "    mos_list = [i.movie_id for i in usr_rev_mos.collect()]\n",
    "\n",
    "    # get movies similar to a list\n",
    "    sim_mos_dfs = []\n",
    "    for i in mos_list:\n",
    "     sim_mos_df = getSimilarMovies(i, sim_mos_limit)\n",
    "     sim_mos_dfs.append(sim_mos_df)\n",
    "\n",
    "     # Change to a DataFrame  \n",
    "    sim_mos_df = sim_mos_dfs[0]\n",
    "    for i in range(1, len(sim_mos_dfs)):\n",
    "     sim_mos_df = sim_mos_df.union(sim_mos_dfs[i])\n",
    "\n",
    "    # filter out those have been reviewd before by the user\n",
    "    a = sim_mos_df.alias(\"a\")\n",
    "    b = usr_rev_mos.alias(\"b\")\n",
    "    c = a.join(b, col(\"a.movie_id\") == col(\"b.movie_id\"), 'left_outer') \\\n",
    "         .where(col(\"b.movie_id\").isNull()) \\\n",
    "         .select([col('a.movie_id'),col('a.score')]).orderBy(\"a.score\", ascending = False)\n",
    "\n",
    "    x = c.limit(sim_mos_limit)\n",
    "\n",
    "    return getMovieDetails(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getContentRecoms(3).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
