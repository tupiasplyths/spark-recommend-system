{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/27 09:19:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/27 09:19:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark3.5/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project\") \\\n",
    "    .master(\"spark://10.10.28.60:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "    # .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    # .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    \n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data Preparation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movies = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/movies.parquet')\n",
    "ratings = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/ratings.parquet')\n",
    "tags = spark.read.parquet('hdfs://master5:9000/user/dis/movielens/tags.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_movies = movies\n",
    "df_ratings = ratings\n",
    "df_tags = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_movies.createOrReplaceTempView(\"movies\")\n",
    "df_ratings.createOrReplaceTempView(\"ratings\")\n",
    "df_tags.createOrReplaceTempView(\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_ratings = df_ratings.drop('timestamp')\n",
    "df_ratings = df_ratings.dropna(subset=['userId', 'movieId'])\n",
    "df_ratings = df_ratings.withColumn(\"userId\", df_ratings[\"userId\"].cast(\"int\"))\n",
    "df_ratings = df_ratings.withColumn(\"movieId\", df_ratings[\"movieId\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'hdfs://master5:9000/user/dis/output-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "(train, test) = df_ratings.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 15:56:10 ERROR TaskSchedulerImpl: Lost executor 1 on 10.10.28.60: Command exited with code 137\n",
      "24/05/26 15:56:10 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 13) (10.10.28.60 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/05/26 15:56:10 WARN TaskSetManager: Lost task 5.0 in stage 5.0 (TID 16) (10.10.28.60 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/05/26 15:56:10 WARN TaskSetManager: Lost task 2.1 in stage 5.0 (TID 18) (10.10.28.61 executor 2): FetchFailed(BlockManagerId(1, 10.10.28.60, 36701, None), shuffleId=1, mapIndex=0, mapId=4, reduceId=2, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1233)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 1), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1203)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1195)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:715)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\t... 30 more\n",
      "\n",
      ")\n",
      "24/05/26 15:56:10 WARN TaskSetManager: Lost task 5.1 in stage 5.0 (TID 17) (10.10.28.61 executor 2): FetchFailed(BlockManagerId(1, 10.10.28.60, 36701, None), shuffleId=1, mapIndex=0, mapId=4, reduceId=5, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1233)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 1), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1203)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1195)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:715)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\t... 30 more\n",
      "\n",
      ")\n",
      "24/05/26 15:56:10 WARN TaskSetManager: Lost task 6.0 in stage 5.0 (TID 19) (10.10.28.61 executor 2): FetchFailed(BlockManagerId(1, 10.10.28.60, 36701, None), shuffleId=1, mapIndex=0, mapId=4, reduceId=6, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:437)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1233)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:971)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\n",
      "\tat org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:124)\n",
      "\tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:106)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 1), which maintains the block data to fetch is dead.\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:141)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:374)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1203)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1195)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:715)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\t... 30 more\n",
      "\n",
      ")\n",
      "24/05/26 15:57:19 ERROR TaskSchedulerImpl: Lost executor 3 on 10.10.28.60: Command exited with code 137\n",
      "24/05/26 15:57:19 WARN TaskSetManager: Lost task 2.0 in stage 57.0 (TID 274) (10.10.28.60 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/05/26 15:57:19 WARN TaskSetManager: Lost task 5.0 in stage 57.0 (TID 277) (10.10.28.60 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/05/26 15:57:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_9 !\n",
      "24/05/26 15:57:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_222_8 !\n",
      "[Stage 115:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.8063740141561199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "alsb = ALS(rank=10, maxIter=10, regParam=0.03, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", \\\n",
    "               coldStartStrategy=\"drop\")\n",
    "alsb_model = alsb.fit(train)\n",
    "\n",
    "alsb_predictions = alsb_model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(alsb_predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# save the ALS model\n",
    "#alsb_model.save(model_path + 'als')\n",
    "\n",
    "# 20,5,0.1 Root-mean-square error = 0.8226567600924868\n",
    "# 10,10,0.3 Root-mean-square error = 0.9065847323471237\n",
    "# 10,10,0.2 Root-mean-square error = 0.8576249876087348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsb_model = ALSModel.load(model_path + 'als')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsn_model = ALSModel.read().load(model_path+ 'als')\n",
    "\n",
    "#Get 5 recommends for user\n",
    "userRecoms = alsn_model.recommendForAllUsers(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{175275, 6.86276...|\n",
      "|     3|[{200930, 7.04948...|\n",
      "|     5|[{193063, 6.79898...|\n",
      "|     6|[{225435, 7.52915...|\n",
      "|     9|[{225435, 5.98800...|\n",
      "|    12|[{225435, 8.80683...|\n",
      "|    13|[{178501, 7.54789...|\n",
      "|    15|[{193063, 6.26425...|\n",
      "|    16|[{216663, 7.45282...|\n",
      "|    17|[{126941, 8.21922...|\n",
      "|    19|[{178727, 5.24674...|\n",
      "|    20|[{199187, 8.11858...|\n",
      "|    22|[{160824, 6.26756...|\n",
      "|    26|[{173651, 6.97203...|\n",
      "|    27|[{173651, 6.18857...|\n",
      "|    28|[{116847, 5.61974...|\n",
      "|    31|[{177209, 5.32568...|\n",
      "|    34|[{185211, 6.33270...|\n",
      "|    35|[{200930, 5.75171...|\n",
      "|    37|[{222368, 6.95421...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "userRecoms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "userRecoms.write.mode('overwrite').parquet(model_path + \"recom_als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{175275, 6.86276...|\n",
      "|     3|[{200930, 7.04948...|\n",
      "|     5|[{193063, 6.79898...|\n",
      "|     6|[{225435, 7.52915...|\n",
      "|     9|[{225435, 5.98800...|\n",
      "|    12|[{225435, 8.80683...|\n",
      "|    13|[{178501, 7.54789...|\n",
      "|    15|[{193063, 6.26425...|\n",
      "|    16|[{216663, 7.45282...|\n",
      "|    17|[{126941, 8.21922...|\n",
      "|    19|[{178727, 5.24674...|\n",
      "|    20|[{199187, 8.11858...|\n",
      "|    22|[{160824, 6.26756...|\n",
      "|    26|[{173651, 6.97203...|\n",
      "|    27|[{173651, 6.18857...|\n",
      "|    28|[{116847, 5.61974...|\n",
      "|    31|[{177209, 5.32568...|\n",
      "|    34|[{185211, 6.33270...|\n",
      "|    35|[{200930, 5.75171...|\n",
      "|    37|[{222368, 6.95421...|\n",
      "|    40|[{173655, 7.05399...|\n",
      "|    41|[{225435, 6.02123...|\n",
      "|    43|[{222368, 7.00660...|\n",
      "+------+--------------------+\n",
      "only showing top 23 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendation = spark.read.parquet(model_path + \"recom_als\")\n",
    "recommendation.show(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user_id):\n",
    "    recs = recommendation.filter(col(\"userId\") == user_id).select(\"recommendations\")\n",
    "    recs = recs.select(explode(col(\"recommendations\")).alias(\"rec\")).select(\"rec.movieId\", \"rec.rating\")\n",
    "    item_list = recs.orderBy(col(\"rating\").desc()).select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "    return item_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend movies for user 1: \n",
      "[175275, 126941, 225435, 225429, 222368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend movies for user 2: \n",
      "[229615, 176657, 183053, 181873, 179063]\n",
      "Recommend movies for user 3: \n",
      "[200930, 178501, 125938, 231289, 231287]\n",
      "Recommend movies for user 4: \n",
      "[216663, 173655, 206160, 104119, 80195]\n",
      "Recommend movies for user 5: \n",
      "[193063, 160824, 236067, 225435, 225429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend movies for user 6: \n",
      "[225435, 225429, 225437, 225425, 126941]\n",
      "Recommend movies for user 7: \n",
      "[126941, 175275, 225435, 225429, 222368]\n",
      "Recommend movies for user 8: \n",
      "[193063, 122015, 236067, 7699, 160824]\n",
      "Recommend movies for user 9: \n",
      "[225435, 225429, 222368, 126941, 175275]\n",
      "Recommend movies for user 10: \n",
      "[126941, 193063, 225435, 225429, 160824]\n",
      "Recommend movies for user 11: \n",
      "[240054, 160824, 222368, 193817, 231289]\n",
      "Recommend movies for user 12: \n",
      "[225435, 225429, 225437, 225425, 147734]\n",
      "Recommend movies for user 13: \n",
      "[178501, 160824, 200930, 222368, 98221]\n",
      "Recommend movies for user 14: \n",
      "[160824, 214148, 148886, 98221, 248830]\n",
      "Recommend movies for user 15: \n",
      "[193063, 155020, 228769, 215635, 160824]\n",
      "Recommend movies for user 16: \n",
      "[216663, 175275, 169604, 125938, 175625]\n",
      "Recommend movies for user 17: \n",
      "[126941, 178501, 175275, 181333, 185373]\n",
      "Recommend movies for user 18: \n",
      "[236067, 223978, 225435, 225429, 173651]\n",
      "Recommend movies for user 19: \n",
      "[178727, 182521, 222007, 243374, 191943]\n",
      "Recommend movies for user 20: \n",
      "[199187, 126941, 178501, 175275, 136473]\n",
      "Recommend movies for user 21: \n",
      "[160824, 222368, 126941, 178501, 193817]\n",
      "Recommend movies for user 22: \n",
      "[160824, 200930, 193817, 159017, 227300]\n",
      "Recommend movies for user 23: \n",
      "[139060, 185211, 160824, 80195, 200930]\n",
      "Recommend movies for user 24: \n",
      "[222368, 200930, 231289, 231287, 219031]\n",
      "Recommend movies for user 25: \n",
      "[215635, 141046, 110752, 183135, 171631]\n",
      "Recommend movies for user 26: \n",
      "[173651, 213219, 160824, 185211, 180223]\n",
      "Recommend movies for user 27: \n",
      "[173651, 176569, 213219, 236067, 200930]\n",
      "Recommend movies for user 28: \n",
      "[116847, 138580, 216663, 251396, 176569]\n",
      "Recommend movies for user 29: \n",
      "[175275, 225435, 225429, 222368, 84996]\n",
      "Recommend movies for user 30: \n",
      "[193817, 200930, 160824, 194434, 193918]\n",
      "Recommend movies for user 31: \n",
      "[177209, 204078, 222368, 157334, 32851]\n",
      "Recommend movies for user 32: \n",
      "[225435, 225429, 171295, 206799, 139060]\n",
      "Recommend movies for user 33: \n",
      "[200930, 160824, 222368, 193817, 231289]\n",
      "Recommend movies for user 34: \n",
      "[185211, 193063, 236067, 158755, 200930]\n",
      "Recommend movies for user 35: \n",
      "[200930, 225435, 225429, 286911, 178501]\n",
      "Recommend movies for user 36: \n",
      "[178501, 200414, 142871, 217721, 175275]\n",
      "Recommend movies for user 37: \n",
      "[222368, 231289, 231287, 219031, 194434]\n",
      "Recommend movies for user 38: \n",
      "[179063, 200486, 208459, 116847, 220108]\n",
      "Recommend movies for user 39: \n",
      "[185211, 215635, 193063, 215655, 99493]\n",
      "Recommend movies for user 40: \n",
      "[173655, 169908, 206160, 265364, 260485]\n",
      "Recommend movies for user 41: \n",
      "[225435, 225429, 222368, 126941, 175275]\n",
      "Recommend movies for user 42: \n",
      "[218211, 185211, 193063, 181705, 139060]\n",
      "Recommend movies for user 43: \n",
      "[222368, 193817, 154402, 160824, 227300]\n",
      "Recommend movies for user 44: \n",
      "[160824, 159017, 177209, 131166, 138066]\n",
      "Recommend movies for user 45: \n",
      "[200930, 204304, 134673, 286911, 227300]\n",
      "Recommend movies for user 46: \n",
      "[193063, 185211, 236067, 155591, 213764]\n",
      "Recommend movies for user 47: \n",
      "[178727, 185669, 204304, 181853, 240054]\n",
      "Recommend movies for user 48: \n",
      "[185211, 236067, 193063, 220484, 173651]\n",
      "Recommend movies for user 49: \n",
      "[180851, 236067, 193063, 170613, 182759]\n",
      "Recommend movies for user 50: \n",
      "[173651, 159017, 176569, 138066, 116847]\n",
      "Recommend movies for user 51: \n",
      "[222368, 225435, 225429, 175275, 225437]\n",
      "Recommend movies for user 52: \n",
      "[221188, 160183, 240054, 193817, 185645]\n",
      "Recommend movies for user 53: \n",
      "[225435, 225429, 222368, 200930, 236067]\n",
      "Recommend movies for user 54: \n",
      "[193063, 185211, 236067, 200930, 205587]\n",
      "Recommend movies for user 55: \n",
      "[162436, 176657, 211241, 168124, 183053]\n",
      "Recommend movies for user 56: \n",
      "[225435, 225429, 178727, 222368, 196559]\n",
      "Recommend movies for user 57: \n",
      "[225435, 225429, 126941, 193063, 222368]\n",
      "Recommend movies for user 58: \n",
      "[160824, 193063, 7699, 162984, 193817]\n",
      "Recommend movies for user 59: \n",
      "[182759, 180851, 175625, 103871, 236067]\n",
      "Recommend movies for user 60: \n",
      "[218211, 196559, 284435, 160183, 278166]\n",
      "Recommend movies for user 61: \n",
      "[81898, 144202, 253912, 147734, 240772]\n",
      "Recommend movies for user 62: \n",
      "[160824, 225435, 225429, 193063, 200930]\n",
      "Recommend movies for user 63: \n",
      "[217881, 195157, 199890, 171595, 220924]\n",
      "Recommend movies for user 64: \n",
      "[200930, 178501, 231289, 231287, 219031]\n",
      "Recommend movies for user 65: \n",
      "[126941, 222368, 175275, 178501, 125938]\n",
      "Recommend movies for user 66: \n",
      "[175275, 126941, 178501, 222368, 225435]\n",
      "Recommend movies for user 67: \n",
      "[185211, 236067, 193063, 173651, 200930]\n",
      "Recommend movies for user 68: \n",
      "[225435, 225429, 126941, 193063, 225437]\n",
      "Recommend movies for user 69: \n",
      "[222368, 225435, 225429, 178501, 126941]\n",
      "Recommend movies for user 70: \n",
      "[169908, 222368, 181333, 166221, 190707]\n",
      "Recommend movies for user 71: \n",
      "[287705, 147292, 216663, 189357, 265656]\n",
      "Recommend movies for user 72: \n",
      "[178501, 175275, 222368, 227300, 200930]\n",
      "Recommend movies for user 73: \n",
      "[222368, 175275, 199187, 178501, 149116]\n",
      "Recommend movies for user 74: \n",
      "[212104, 158755, 151277, 123941, 278380]\n",
      "Recommend movies for user 75: \n",
      "[222368, 231289, 231287, 219031, 200930]\n",
      "Recommend movies for user 76: \n",
      "[225435, 225429, 200930, 193063, 205587]\n",
      "Recommend movies for user 77: \n",
      "[159017, 160824, 142871, 177209, 210775]\n",
      "Recommend movies for user 78: \n",
      "[185211, 176569, 116847, 169632, 200930]\n",
      "Recommend movies for user 79: \n",
      "[160824, 200930, 193817, 240054, 194434]\n",
      "Recommend movies for user 80: \n",
      "[116847, 176569, 96651, 220558, 130292]\n",
      "Recommend movies for user 81: \n",
      "[225435, 225429, 204304, 175275, 200930]\n",
      "Recommend movies for user 82: \n",
      "[229615, 116403, 241604, 240054, 225435]\n",
      "Recommend movies for user 83: \n",
      "[181333, 199187, 136892, 126941, 166747]\n",
      "Recommend movies for user 84: \n",
      "[206799, 185211, 213219, 188925, 236067]\n",
      "Recommend movies for user 85: \n",
      "[204972, 175275, 162436, 177523, 86952]\n",
      "Recommend movies for user 86: \n",
      "[185211, 215635, 173651, 200930, 213219]\n",
      "Recommend movies for user 87: \n",
      "[214910, 160824, 155020, 144202, 93918]\n",
      "Recommend movies for user 88: \n",
      "[199187, 175275, 126941, 225435, 225429]\n",
      "Recommend movies for user 89: \n",
      "[236067, 160824, 158755, 156414, 175157]\n",
      "Recommend movies for user 90: \n",
      "[175731, 236067, 227066, 194434, 193918]\n",
      "Recommend movies for user 91: \n",
      "[199187, 178501, 211241, 181333, 217721]\n",
      "Recommend movies for user 92: \n",
      "[146112, 274525, 156799, 187049, 177039]\n",
      "Recommend movies for user 93: \n",
      "[193063, 93918, 126941, 167044, 214910]\n",
      "Recommend movies for user 94: \n",
      "[116847, 176569, 159017, 204304, 200930]\n",
      "Recommend movies for user 95: \n",
      "[236067, 158755, 156414, 182851, 200930]\n",
      "Recommend movies for user 96: \n",
      "[217721, 148886, 215003, 219573, 264554]\n",
      "Recommend movies for user 97: \n",
      "[225435, 225429, 225437, 225425, 178727]\n",
      "Recommend movies for user 98: \n",
      "[260485, 200872, 199790, 198591, 173393]\n",
      "Recommend movies for user 99: \n",
      "[216663, 190297, 149450, 179063, 202191]\n",
      "Recommend movies for user 100: \n",
      "[69609, 173655, 167384, 222368, 178501]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 101):\n",
    "    result = get_recommendations(i)\n",
    "    print(f'Recommend movies for user {i}: ')\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecoms.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
